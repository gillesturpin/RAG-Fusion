# Documentation Technique - Agentic RAG

**Version** : 2.0.0
**Date** : Novembre 2025
**Impl√©mentation** : Pure LangChain/LangGraph selon [tutoriel officiel](https://python.langchain.com/docs/tutorials/rag_agent/)

## Table des mati√®res
1. [Stack Technique](#stack-technique)
2. [Structure du Projet](#structure-du-projet)
3. [Impl√©mentation RAG Agent](#impl√©mentation-rag-agent)
4. [API REST](#api-rest)
5. [Mode Stateless (Configuration Actuelle)](#mode-stateless-configuration-actuelle)
6. [Streaming](#streaming)
7. [Upload de Documents](#upload-de-documents)
8. [Tests](#tests)
9. [D√©ploiement](#d√©ploiement)

---

## Stack Technique

### Backend
- **Python** 3.12+
- **FastAPI** - Framework web async
- **LangChain** 0.3.x - Orchestration LLM
- **LangGraph** 0.2.x - State machine workflow
- **Anthropic** Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- **ChromaDB** - Vector store (local persistence)
- **HuggingFace** - Embeddings (`sentence-transformers/all-MiniLM-L6-v2`)
- **Stateless mode** - No conversation memory (checkpointer=None by default)

### Frontend
- **React** 19
- **Vite** 7.x - Build tool
- **Axios** - HTTP client
- **Framer Motion** - Animations
- **React Markdown** - Rendu markdown
- **Recharts** - Graphiques

### Infrastructure
- **Docker** & **Docker Compose**
- **Uvicorn** - ASGI server
- **CORS** - Middleware FastAPI

---

## Structure du Projet

```
agentic-rag/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py              # FastAPI server (480 lignes)
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ rags/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py           # Package exports
‚îÇ       ‚îî‚îÄ‚îÄ rag_agent.py         # RAG Agent avec RAG Fusion (327 lignes)
‚îÇ
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx              # Application React
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile               # Build React
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ chroma_db/               # Persistance ChromaDB
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md          # Architecture d√©taill√©e
‚îÇ   ‚îî‚îÄ‚îÄ DOCUMENTATION_TECHNIQUE.md # Ce fichier
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml           # Orchestration
‚îú‚îÄ‚îÄ Dockerfile                   # Build backend
‚îú‚îÄ‚îÄ requirements.txt             # D√©pendances Python
‚îú‚îÄ‚îÄ test_memory.py               # Tests m√©moire
‚îî‚îÄ‚îÄ README.md                    # Documentation
```

---

## Impl√©mentation RAG Agent

### Fichier : `backend/rags/rag_agent.py`

Le RAG Agent suit le tutoriel officiel LangChain avec deux am√©liorations majeures :
1. **RAG Fusion** : Multi-query retrieval avec RRF reranking pour am√©liorer la qualit√© de r√©cup√©ration
2. **Mode stateless** : Pas de m√©moire conversationnelle par d√©faut (optimis√© pour √©valuation)

### 1. Initialisation

Voir le code r√©el dans `backend/rags/rag_agent.py` (lignes 75-131).

**Signature** :
```python
def __init__(self, vectorstore, checkpointer=None, use_rag_fusion=True, temperature=1.0, k_documents=8)
```

**Configuration par d√©faut** :
- **checkpointer=None** : Mode stateless, aucune m√©moire entre questions
- **use_rag_fusion=True** : RAG Fusion activ√© (multi-query + RRF)
- **temperature=1.0** : Temp√©rature maximale pour g√©n√©ration cr√©ative
- **k_documents=8** : Nombre final de documents retourn√©s apr√®s fusion

**Fonctionnement RAG Fusion** :
1. G√©n√®re 4 variations de la question (1 originale + 3 reformulations)
2. R√©cup√®re 4 documents pour chaque variation (total : 16 documents)
3. Applique RRF (Reciprocal Rank Fusion) pour reranker
4. Retourne les top k=8 documents avec meilleurs scores

**LLM** :
- **Mod√®le** : Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- **Temperature** : 1.0 (configurable)
- **Tools** : Fonction `retrieve` avec RAG Fusion

**Points cl√©s** :
- **Vectorstore** : ChromaDB pass√© en param√®tre (cr√©√© au startup)
- **Checkpointer** : None par d√©faut = AUCUNE m√©moire conversationnelle
- **Tool** : Fonction `retrieve` avec logique RAG Fusion int√©gr√©e
- **Graph** : Compil√© sans checkpointer (stateless)

### 2. Build du Graph LangGraph

```python
def _build_graph(self):
    """Construit le StateGraph LangGraph"""
    workflow = StateGraph(MessagesState)

    # Add nodes
    workflow.add_node("agent", self._call_model)
    workflow.add_node("tools", ToolNode(self.tools))

    # Entry point
    workflow.add_edge(START, "agent")

    # Conditional routing
    def should_continue(state: MessagesState) -> Literal["tools", END]:
        messages = state["messages"]
        last_message = messages[-1]
        # Si tool_calls pr√©sents ‚Üí execute tools
        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            return "tools"
        # Sinon ‚Üí fin
        return END

    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            END: END,
        }
    )

    # Loop: tools ‚Üí agent
    workflow.add_edge("tools", "agent")

    # Compile (stateless si checkpointer=None)
    if self.checkpointer:
        return workflow.compile(checkpointer=self.checkpointer)
    else:
        return workflow.compile()  # Stateless par d√©faut
```

**Flow** :
1. START ‚Üí agent
2. Agent d√©cide : tool call ou r√©ponse directe
3. Si tool call ‚Üí tools ‚Üí agent (loop)
4. Si pas de tool call ‚Üí END

### 3. Call Model avec Trimming

```python
def _call_model(self, state: MessagesState):
    """Invoke LLM avec message trimming"""
    messages = state["messages"]

    # Trim pour √©viter overflow du contexte
    messages = trim_messages(messages)

    # Ajouter system message si absent
    if not messages or messages[0].type != "system":
        system_msg = SystemMessage(
            "You have access to a tool that retrieves context from documents. "
            "Use the tool to help answer user queries. "
            "IMPORTANT: Provide COMPLETE and COMPREHENSIVE answers."
        )
        messages = [system_msg] + messages

    # Appel LLM avec tools
    response = self.model_with_tools.invoke(messages)
    return {"messages": [response]}
```

**Trimming** :
```python
def trim_messages(messages):
    """Garde les 10 derniers messages pour ne pas d√©passer le contexte"""
    if len(messages) <= 10:
        return messages
    # Garde le premier (system) + 9 derniers
    return [messages[0]] + messages[-9:]
```

### 4. Invoke (Stateless par d√©faut)

Voir le code r√©el dans `backend/rags/rag_agent.py` (lignes 254-288).

**Signature** :
```python
def invoke(self, question: str, thread_id: str = None) -> dict
```

**IMPORTANT** : En mode stateless (checkpointer=None), le param√®tre `thread_id` est **IGNOR√â**.

```python
# Config pour m√©moire (SEULEMENT si checkpointer existe)
config = {}
if thread_id and self.checkpointer:  # <- Note le AND
    config = {"configurable": {"thread_id": thread_id}}
```

**Comportement actuel (checkpointer=None)** :
- Chaque question est **ind√©pendante**
- Aucune m√©moire entre les appels
- thread_id est retourn√© mais n'a aucun effet
- Optimis√© pour √©valuation RAGAS (pas de contamination entre questions)

**Retour** :
```python
{
    "answer": str,           # R√©ponse g√©n√©r√©e
    "messages": list,        # Liste des messages (HumanMessage, AIMessage, ToolMessage)
    "used_retrieval": bool,  # True si le tool retrieve a √©t√© appel√©
    "thread_id": str         # Retourn√© tel quel (tracking frontend uniquement)
}
```

---

## API REST

### Fichier : `backend/api/main.py`

### Startup Event

```python
@app.on_event("startup")
async def startup_event():
    """Initialize RAG agent on startup"""
    global rag_agent, vectorstore

    # Initialize embeddings
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    # Load ChromaDB
    persist_dir = Path(__file__).parent.parent.parent / "data" / "chroma_db"
    vectorstore = Chroma(
        persist_directory=str(persist_dir),
        embedding_function=embeddings
    )

    # Initialize RAG agent (STATELESS mode)
    rag_agent = RAGAgent(vectorstore, checkpointer=None)
    # Par d√©faut : use_rag_fusion=True, temperature=1.0, k_documents=8

    print("‚úì RAG Agent initialized in STATELESS mode with RAG Fusion")
```

### Endpoints Principaux

#### 1. `POST /api/rag_agent`

Endpoint natif pour le RAG Agent.

```python
@app.post("/api/rag_agent", response_model=AgentResponse)
async def query_rag_agent(request: QueryRequest):
    """Query the RAG agent (STATELESS - thread_id ignored)"""
    start_time = time.time()

    try:
        # Generate thread_id si absent (pour tracking frontend uniquement)
        thread_id = request.thread_id or str(uuid.uuid4())

        # Invoke (thread_id IGNOR√â car checkpointer=None)
        result = rag_agent.invoke(request.question, thread_id=thread_id)
        latency = time.time() - start_time

        return AgentResponse(
            answer=result["answer"],
            used_retrieval=result.get("used_retrieval"),
            latency=latency,
            thread_id=result.get("thread_id")
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**Request** :
```json
{
  "question": "What is RAG?",
  "thread_id": "thread-abc123"  // Optionnel
}
```

**Response** :
```json
{
  "answer": "RAG stands for...",
  "used_retrieval": true,
  "latency": 2.45,
  "thread_id": "thread-abc123"
}
```

#### 2. `POST /api/query`

Endpoint adapter pour compatibilit√© frontend.

```python
@app.post("/api/query")
async def query_adapter(request: dict):
    """Adapter pour frontend original (STATELESS)"""
    question = request.get("question", "")
    session_id = request.get("session_id", str(uuid.uuid4()))

    # Mapping session_id ‚Üí thread_id (INUTILE car checkpointer=None)
    # Gard√© uniquement pour compatibilit√© frontend
    if session_id not in session_threads:
        session_threads[session_id] = f"thread-{uuid.uuid4()}"
    thread_id = session_threads[session_id]

    # Invoke agent (thread_id IGNOR√â)
    result = rag_agent.invoke(question, thread_id=thread_id)

    return {
        "answer": result["answer"],
        "method": "optimized_rag_agent",
        "latency": ...,
        "confidence": 0.90,  # Hardcod√©
        "faithfulness": 0.92,  # Hardcod√©
        "thread_id": thread_id  # Retourn√© mais sans effet
    }
```

**Notes** :
- `confidence` et `faithfulness` sont **hardcod√©s**
- Le mapping session_id ‚Üí thread_id est **inutile** (checkpointer=None) mais gard√© pour compatibilit√©

#### 3. `POST /api/upload`

Upload et traitement de documents.

```python
@app.post("/api/upload")
async def upload_documents(files: List[UploadFile] = File(...)):
    """Upload PDF, TXT, MD, DOCX, IPYNB"""

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,
        chunk_overlap=400
    )

    for file in files:
        # Save temp
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            content = await file.read()
            tmp.write(content)
            tmp_path = tmp.name

        # Load selon extension
        if file.filename.endswith('.pdf'):
            loader = PyPDFLoader(tmp_path)
        elif file.filename.endswith(('.txt', '.md')):
            loader = TextLoader(tmp_path)
        elif file.filename.endswith('.docx'):
            loader = UnstructuredWordDocumentLoader(tmp_path)
        elif file.filename.endswith('.ipynb'):
            loader = NotebookLoader(tmp_path)

        documents = loader.load()

        # Split
        chunks = text_splitter.split_documents(documents)

        # Add metadata
        for chunk in chunks:
            chunk.metadata["source"] = file.filename
            chunk.metadata["upload_date"] = time.strftime("%Y-%m-%d %H:%M:%S")

        # Add to vectorstore
        vectorstore.add_documents(chunks)

    return {"total_chunks": total_chunks}
```

---

## Mode Stateless (Configuration Actuelle)

### Pas de M√©moire Conversationnelle

Le syst√®me est configur√© en **mode stateless** par d√©faut (`checkpointer=None`).

**Implications** :
- ‚ùå **Aucune m√©moire** entre les questions
- ‚ùå Le param√®tre `thread_id` est **accept√© mais IGNOR√â**
- ‚úÖ Chaque question est **totalement ind√©pendante**
- ‚úÖ Optimis√© pour **√©valuation RAGAS** (pas de contamination entre questions)
- ‚úÖ Pas de stockage d'√©tat en m√©moire

**Exemple de comportement actuel** :

```python
# Question 1
agent.invoke("My name is Alice", thread_id="thread-1")
# ‚Üí R√©ponse g√©n√©r√©e

# Question 2 (M√äME thread_id)
agent.invoke("What is my name?", thread_id="thread-1")
# ‚Üí "I don't have information about your name" ‚ùå Pas de m√©moire !

# Le thread_id ne fait RIEN car checkpointer=None
```

### Activer la M√©moire Conversationnelle (Non impl√©ment√© actuellement)

Pour activer la m√©moire, il faudrait :

1. **Changer le code dans `backend/api/main.py`** :
```python
from langgraph.checkpoint.memory import MemorySaver

# Ligne 77 (actuellement : checkpointer=None)
checkpointer = MemorySaver()  # Au lieu de None
rag_agent = RAGAgent(vectorstore, checkpointer=checkpointer)
```

2. **Fonctionnement avec m√©moire** :
- Chaque `thread_id` aurait son propre √©tat isol√©
- L'historique des messages serait sauvegard√©
- Pas de limite de dur√©e (reste en RAM)

3. **Pour persistance en production** :
- Utiliser `SqliteSaver` ou `PostgresSaver` au lieu de `MemorySaver`
- Voir [LangGraph Checkpointers](https://langchain-ai.github.io/langgraph/reference/checkpoints/)

**Note** : La m√©moire n'est PAS activ√©e pour optimiser l'√©valuation RAGAS.

---

## Streaming

### Server-Sent Events (SSE)

Endpoint streaming pour r√©ponses en temps r√©el.

```python
@app.post("/api/query/stream")
async def query_stream(request: dict):
    """Streaming SSE"""
    question = request.get("question", "")
    thread_id = ...

    return StreamingResponse(
        generate_sse_stream(question, thread_id),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
        }
    )
```

### G√©n√©rateur SSE

```python
async def generate_sse_stream(question: str, thread_id: str):
    """Generate SSE events"""

    # Get full response
    result = rag_agent.invoke(question, thread_id)
    full_response = result["answer"]

    # Simulate streaming (mot par mot)
    words = full_response.split()
    for word in words:
        event_data = {
            "type": "token",
            "content": word + " ",
            "thread_id": thread_id
        }
        yield f"data: {json.dumps(event_data)}\n\n"
        await asyncio.sleep(0.03)  # 30ms delay

    # Completion event
    event_data = {
        "type": "complete",
        "content": full_response,
        "thread_id": thread_id
    }
    yield f"data: {json.dumps(event_data)}\n\n"
```

**Note** : LangGraph ne supporte pas encore le streaming token-by-token natif, d'o√π la simulation.

---

## Upload de Documents

### Formats Support√©s

- **PDF** : `PyPDFLoader`
- **TXT/MD** : `TextLoader`
- **DOCX** : `UnstructuredWordDocumentLoader`
- **IPYNB** : `NotebookLoader` (avec outputs)

### Text Splitting

```python
RecursiveCharacterTextSplitter(
    chunk_size=2000,      # Chunks plus gros pour meilleur contexte
    chunk_overlap=400,    # 20% overlap
    separators=["\n\n", "\n", " ", ""]
)
```

### Ajout Vectorstore

```python
# Add chunks avec m√©tadonn√©es
vectorstore.add_documents(chunks)

# ChromaDB persiste automatiquement
```

---

## Tests

### Test Mode Stateless

**Fichier** : `test_memory.py`

```bash
python test_memory.py
```

**Tests actuels (mode stateless)** :
- ‚ùå Threads diff√©rents ‚Üí pas de m√©moire (normal)
- ‚ùå M√™me thread ‚Üí **PAS de m√©moire** (car checkpointer=None)
- ‚úÖ Trimming ‚Üí garde 10 messages max
- ‚úÖ RAG Fusion ‚Üí g√©n√®re 4 queries, r√©cup√®re 16 docs, retourne top 8

### Test API Manuel

```bash
# Lancer serveur
cd backend/api && python main.py

# Test basique (stateless)
curl -X POST http://localhost:8000/api/rag_agent \
  -H "Content-Type: application/json" \
  -d '{"question": "What is RAG?"}'

# Test avec thread_id (IGNOR√â car checkpointer=None)
curl -X POST http://localhost:8000/api/rag_agent \
  -H "Content-Type: application/json" \
  -d '{"question": "My name is Bob", "thread_id": "test-1"}'

curl -X POST http://localhost:8000/api/rag_agent \
  -H "Content-Type: application/json" \
  -d '{"question": "What is my name?", "thread_id": "test-1"}'
# ‚Üí "I don't have information about your name" ‚ùå PAS de m√©moire !
# Le thread_id est accept√© mais IGNOR√â (checkpointer=None)
```

**Note** : Pour tester la m√©moire, il faudrait d'abord modifier `backend/api/main.py` ligne 77 pour activer un checkpointer.

---

## D√©ploiement

### Variables d'Environnement

```bash
# .env
ANTHROPIC_API_KEY=sk-ant-...   # Requis
TAVILY_API_KEY=tvly-...        # Optionnel (non utilis√©)
```

### Docker Compose

```yaml
version: '3.8'

services:
  backend:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
    volumes:
      - ./data:/app/data

  frontend:
    build: ./frontend
    ports:
      - "7860:80"
    depends_on:
      - backend
```

### Lancement

```bash
docker-compose up --build
```

**Acc√®s** :
- Backend : http://localhost:8000
- Frontend : http://localhost:7860
- Docs API : http://localhost:8000/docs

---

## M√©triques & √âvaluation RAGAS

### Syst√®me d'√âvaluation Actuel

Le syst√®me utilise **RAGAS 0.3.9** pour √©valuation objective avec LLM-as-judge (Claude Haiku).

### M√©triques Principales

**2 m√©triques essentielles** (s√©lectionn√©es apr√®s optimisation) :

| M√©trique | Poids | Description | Seuil Pass |
|----------|-------|-------------|------------|
| **Context Precision** | 30% | Qualit√© du retrieval - Documents pertinents bien class√©s | ‚â• 0.80 |
| **Answer Similarity** | 70% | Similarit√© s√©mantique answer vs ground truth | ‚â• 0.70 |

**Score Global** = (Context Precision √ó 0.30) + (Answer Similarity √ó 0.70)

### Explication des M√©triques

#### 1. Context Precision (30% du score)

**Qu'est-ce que c'est ?**
- Mesure la **qualit√© du retrieval** : les documents r√©cup√©r√©s sont-ils pertinents ?
- V√©rifie si les documents utilis√©s pour g√©n√©rer la r√©ponse sont **r√©ellement utiles**

**Comment c'est calcul√© ?**
```
Context Precision = Documents pertinents / Total documents r√©cup√©r√©s

Exemple :
- R√©cup√©ration : 8 documents
- Documents pertinents (contiennent l'info n√©cessaire) : 8
- Context Precision = 8/8 = 1.0 (100%) ‚≠ê
```

**Pourquoi c'est important ?**
- ‚úÖ Score √©lev√© (>0.8) = RAG Fusion + RRF fonctionnent bien
- ‚úÖ Pas de "bruit" : seulement des docs pertinents
- ‚ùå Score faible (<0.5) = Beaucoup de docs inutiles r√©cup√©r√©s

**Comment RAGAS l'√©value ?**
1. LLM-as-judge (Claude Haiku) analyse chaque document r√©cup√©r√©
2. Question : "Ce document contient-il des informations utiles pour r√©pondre √† la question ?"
3. Compte le nombre de documents pertinents vs total

**Notre score : 99.99%**
- Signifie : Presque tous les documents r√©cup√©r√©s sont pertinents
- Gr√¢ce √† : RAG Fusion (4 queries) + RRF reranking

---

#### 2. Answer Similarity (70% du score)

**Qu'est-ce que c'est ?**
- Mesure la **similarit√© s√©mantique** entre la r√©ponse g√©n√©r√©e et la r√©ponse de r√©f√©rence (ground truth)
- V√©rifie si le mod√®le a compris et reformul√© correctement l'information

**Comment c'est calcul√© ?**
```
1. Embedding de la r√©ponse g√©n√©r√©e : E(answer)
2. Embedding de la ground truth : E(ground_truth)
3. Similarit√© cosinus : cos(E(answer), E(ground_truth))

Exemple :
Answer: "Git is a version control system for tracking code changes"
Ground truth: "Git is a VCS used to track modifications in source code"
‚Üí Similarity = 0.92 (tr√®s proche s√©mantiquement)
```

**Pourquoi c'est important ?**
- ‚úÖ Score √©lev√© (>0.7) = R√©ponse contient les bonnes informations
- ‚úÖ Tol√©rant aux reformulations (s√©mantique vs exact match)
- ‚ùå Score faible (<0.5) = R√©ponse √† c√¥t√© de la plaque

**Diff√©rence avec exact match :**
| M√©trique | Answer 1 | Answer 2 | Score |
|----------|----------|----------|-------|
| **Exact Match** | "Git is a VCS" | "Git is a version control system" | 0% ‚ùå |
| **Answer Similarity** | "Git is a VCS" | "Git is a version control system" | 95% ‚úÖ |

**Notre score : 82%**
- Signifie : R√©ponses s√©mantiquement tr√®s proches de la v√©rit√©
- Gr√¢ce √† : System prompt optimis√© + Context Precision √©lev√©

---

#### Pourquoi 2 m√©triques seulement ?

**M√©triques RAGAS disponibles** (6 au total) :
1. Context Precision ‚úÖ **Retenue**
2. Answer Similarity ‚úÖ **Retenue**
3. Faithfulness ‚ùå Redondant (d√©j√† couvert par prompt strict)
4. Answer Relevance ‚ùå Captur√© par Answer Similarity
5. Context Recall ‚ùå Difficile √† √©valuer sans annotation manuelle
6. Context Relevance ‚ùå Proche de Context Precision

**D√©cision** :
- Focus sur 2 m√©triques compl√©mentaires
- Context Precision ‚Üí Qualit√© **retrieval**
- Answer Similarity ‚Üí Qualit√© **g√©n√©ration**
- Couvre toute la cha√Æne RAG

**Pond√©ration 30/70** :
```
Score = (Context Precision √ó 0.30) + (Answer Similarity √ó 0.70)

Pourquoi ?
- Answer Similarity (70%) : m√©trique finale, ce que l'utilisateur voit
- Context Precision (30%) : m√©trique interm√©diaire, mais cruciale
```

---

#### ‚ö†Ô∏è Ground Truth : Concept Cl√©

**Les 2 m√©triques utilisent la ground truth** - c'est normal pour l'√©valuation !

**Qu'est-ce que la ground truth ?**
```json
{
  "question": "What are three key learning objectives for Git course?",
  "ground_truth": "The course aims to teach: 1) Git basics as VCS,
                   2) Using Git with terminal and GitHub,
                   3) Collaboration with agile methods"
}
```

La ground truth = **r√©ponse de r√©f√©rence** annot√©e manuellement par un humain.

**Comment chaque m√©trique utilise la ground truth :**

| M√©trique | Utilisation de Ground Truth | Exemple |
|----------|----------------------------|---------|
| **Context Precision** | LLM-as-judge compare chaque document r√©cup√©r√© avec la question + ground truth pour savoir si le doc est pertinent | Doc contient "Git is a VCS" ‚Üí Pertinent pour ground truth mentionnant "Git basics as VCS" ‚úÖ |
| **Answer Similarity** | Calcul de similarit√© s√©mantique directe entre answer et ground truth | Embeddings: cos(answer, ground_truth) = 0.82 |

**Workflow d'√©valuation complet :**

```
1. Dataset avec ground truth (annot√© manuellement)
   ‚îú‚îÄ Question: "What are Git basics?"
   ‚îî‚îÄ Ground Truth: "Git is a version control system..."

2. RAG g√©n√®re une r√©ponse
   ‚îî‚îÄ Answer: "Based on course materials, Git is a VCS for tracking code changes..."

3. √âvaluation RAGAS
   ‚îú‚îÄ Context Precision: LLM v√©rifie si docs r√©cup√©r√©s contiennent infos de ground truth
   ‚îî‚îÄ Answer Similarity: Compare s√©mantiquement answer vs ground truth

4. Score final
   ‚îî‚îÄ 87.4% (Grade A)
```

**Important : √âvaluation vs Production**

| Mode | Ground Truth ? | Utilisation |
|------|----------------|-------------|
| **√âvaluation** | ‚úÖ N√©cessaire | Mesurer la qualit√© du syst√®me |
| **Production** | ‚ùå Pas disponible | Utilisateurs posent des questions r√©elles |

En production, on ne peut **pas** calculer Context Precision ni Answer Similarity car :
- Pas de ground truth pr√©-annot√©e
- Mais on peut monitorer d'autres m√©triques :
  - Latence
  - Nombre de documents r√©cup√©r√©s
  - Feedback utilisateur (üëç/üëé)

**C'est pourquoi l'√©valuation en amont est cruciale** pour s'assurer que le syst√®me fonctionne bien ! üéØ

### R√©sultats Obtenus

**Performance actuelle** (10 questions, dataset Git/Python) :

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  RAGAS EVALUATION - RESULTS                  ‚ïë
‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£
‚ïë  Context Precision:     99.99%  ‚≠ê           ‚ïë
‚ïë  Answer Similarity:     82.0%               ‚ïë
‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚ïë
‚ïë  Overall Score:         87.4%               ‚ïë
‚ïë  Grade:                 A (Very Good)       ‚ïë
‚ïë  Pass Rate:             90% (9/10)          ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

**Analyse** :
- ‚úÖ **Context Precision quasi-parfait** : RAG Fusion + RRF tr√®s efficace
- ‚úÖ **Answer Similarity bon** : Prompts optimis√©s
- ‚úÖ **Pass Rate > 80%** : Seuil d√©pass√©

### Lancer l'√âvaluation

**√âvaluation compl√®te** (10 questions) :
```bash
cd backend/scripts
python run_evaluation.py
```

**√âvaluation rapide** (2 questions) :
```bash
python run_evaluation.py --limit 2
```

**Options disponibles** :
```bash
# D√©sactiver RAG Fusion (test comparatif)
python run_evaluation.py --no-rag-fusion

# Changer temp√©rature
python run_evaluation.py --temperature 0.5

# Changer nombre de documents
python run_evaluation.py --k-documents 12
```

### Fichiers G√©n√©r√©s

- **Dataset** : `backend/scripts/evaluation_dataset.json` (10 questions)
- **Rapport** : `backend/scripts/evaluation_report_YYYYMMDD_HHMMSS.json`

**Structure du rapport** :
```json
{
  "timestamp": "2025-11-18T10:30:00",
  "evaluation_passed": true,
  "summary": {
    "total_questions": 10,
    "passed": 9,
    "pass_rate": 0.9,
    "overall_score": 0.874,
    "grade": "A (Very Good)"
  },
  "detailed_results": [...]
}
```

### Crit√®res de R√©ussite

| Crit√®re | Seuil | Actuel | Status |
|---------|-------|--------|--------|
| Pass Rate | ‚â• 80% | 90% | ‚úÖ |
| Overall Score | ‚â• 0.70 | 0.874 | ‚úÖ |
| Context Precision | ‚â• 0.80 | 0.999 | ‚úÖ |
| Answer Similarity | ‚â• 0.70 | 0.82 | ‚úÖ |

### Dataset d'√âvaluation

**10 questions** couvrant :
- **Git Basics** (3 questions) - facile/moyen
- **Python Classes** (3 questions) - facile/moyen
- **Python Functions** (2 questions) - moyen
- **Markdown** (2 questions) - facile

Chaque question contient :
```json
{
  "id": "q001",
  "question": "What are three key learning objectives...",
  "ground_truth": "The course aims to teach...",
  "category": "Git Basics",
  "difficulty": "easy",
  "source_file": "01-introduction.md"
}
```

### Optimisations Appliqu√©es

**R√©sultats des tests** (voir tests d'optimisation) :

| Configuration | Score | D√©cision |
|--------------|-------|----------|
| k=4 docs | 0.759 | ‚ùå |
| k=8 docs | **0.874** | ‚úÖ **Retenu** |
| k=12 docs | 0.841 | ‚ùå |
| RAG Fusion ON | **0.874** | ‚úÖ **Retenu** |
| RAG Fusion OFF | 0.782 | ‚ùå |
| Temperature 0.0 | 0.801 | ‚ùå |
| Temperature 1.0 | **0.874** | ‚úÖ **Retenu** |

**Configuration finale optimale** :
```python
RAGFusion(
    vectorstore=vectorstore,
    use_rag_fusion=True,      # Multi-query + RRF
    temperature=1.0,           # Diversit√© des r√©ponses
    k_documents=8              # Top 8 apr√®s RRF
)
```

---

## S√©curit√©

### API Key

```python
# Jamais en dur !
api_key = os.getenv("ANTHROPIC_API_KEY")
if not api_key:
    raise RuntimeError("ANTHROPIC_API_KEY not found")
```

### Input Validation

```python
class QueryRequest(BaseModel):
    question: str
    thread_id: Optional[str] = None

    @validator('question')
    def validate_question(cls, v):
        if not v or len(v) > 5000:
            raise ValueError("Invalid question length")
        return v.strip()
```

### CORS

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # √Ä restreindre en prod
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

---

## Ressources

- [LangChain RAG Agent Tutorial](https://python.langchain.com/docs/tutorials/rag_agent/)
- [LangGraph Docs](https://langchain-ai.github.io/langgraph/)
- [Anthropic Claude API](https://docs.anthropic.com/)
- [ChromaDB Docs](https://docs.trychroma.com/)

---

## Roadmap

### Impl√©ment√© ‚úÖ
- **RAG Agent** avec tool calling (LangGraph)
- **RAG Fusion** : Multi-query retrieval + RRF reranking (4 queries ‚Üí 16 docs ‚Üí top 8)
- **Mode stateless** : Pas de m√©moire conversationnelle (optimis√© pour √©valuation)
- **Claude Sonnet 4.5** : Mod√®le de derni√®re g√©n√©ration
- **Temperature 1.0** : G√©n√©ration cr√©ative maximale
- **Streaming SSE** : R√©ponses en temps r√©el
- **Upload multi-formats** : PDF/TXT/MD/DOCX/IPYNB
- **Message trimming** : Garde 10 derniers messages
- **API REST compl√®te** : FastAPI avec CORS
- **√âvaluation RAGAS** : Score 87.4% - Grade A

### √Ä Venir üîú
- **Mode conversationnel** avec m√©moire (MemorySaver/PostgresSaver)
- **Hybrid search** : Dense + sparse (BM25)
- **Citation tracking** : Sources pr√©cises
- **Token usage tracking** : Co√ªts r√©els
- **Rate limiting** : Protection API
- **Tests automatis√©s** : Coverage complet
