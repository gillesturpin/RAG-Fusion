# Documentation Technique - Agentic RAG

**Version** : 2.0.0
**Date** : Novembre 2025
**Impl√©mentation** : Pure LangChain/LangGraph selon [tutoriel officiel](https://python.langchain.com/docs/tutorials/rag_agent/)

## Table des mati√®res
1. [Stack Technique](#stack-technique)
2. [Structure du Projet](#structure-du-projet)
3. [Impl√©mentation RAG Agent](#impl√©mentation-rag-agent)
4. [API REST](#api-rest)
5. [Mode Stateless (Configuration Actuelle)](#mode-stateless-configuration-actuelle)
6. [Streaming](#streaming)
7. [Upload de Documents](#upload-de-documents)
8. [Tests](#tests)
9. [D√©ploiement](#d√©ploiement)

---

## Stack Technique

### Backend
- **Python** 3.12+
- **FastAPI** - Framework web async
- **LangChain** 0.3.x - Orchestration LLM
- **LangGraph** 0.2.x - State machine workflow
- **Anthropic** Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- **ChromaDB** - Vector store (local persistence)
- **HuggingFace** - Embeddings (`sentence-transformers/all-MiniLM-L6-v2`)
- **Stateless mode** - No conversation memory (checkpointer=None by default)

### Frontend
- **React** 19
- **Vite** 7.x - Build tool
- **Axios** - HTTP client
- **Framer Motion** - Animations
- **React Markdown** - Rendu markdown
- **Recharts** - Graphiques

### Infrastructure
- **Docker** & **Docker Compose**
- **Uvicorn** - ASGI server
- **CORS** - Middleware FastAPI

---

## Structure du Projet

```
agentic-rag/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py              # FastAPI server (480 lignes)
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ rags/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py           # Package exports
‚îÇ       ‚îî‚îÄ‚îÄ rag_agent.py         # RAG Agent avec RAG Fusion (327 lignes)
‚îÇ
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx              # Application React
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile               # Build React
‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ chroma_db/               # Persistance ChromaDB
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md          # Architecture d√©taill√©e
‚îÇ   ‚îî‚îÄ‚îÄ DOCUMENTATION_TECHNIQUE.md # Ce fichier
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml           # Orchestration
‚îú‚îÄ‚îÄ Dockerfile                   # Build backend
‚îú‚îÄ‚îÄ requirements.txt             # D√©pendances Python
‚îú‚îÄ‚îÄ test_memory.py               # Tests m√©moire
‚îî‚îÄ‚îÄ README.md                    # Documentation
```

---

## Impl√©mentation RAG Agent

### Fichier : `backend/rags/rag_agent.py`

Le RAG Agent suit le tutoriel officiel LangChain avec deux am√©liorations majeures :
1. **RAG Fusion** : Multi-query retrieval avec RRF reranking pour am√©liorer la qualit√© de r√©cup√©ration
2. **Mode stateless** : Pas de m√©moire conversationnelle par d√©faut (optimis√© pour √©valuation)

### 1. Initialisation

Voir le code r√©el dans `backend/rags/rag_agent.py` (lignes 75-131).

**Signature** :
```python
def __init__(self, vectorstore, checkpointer=None, use_rag_fusion=True, temperature=1.0, k_documents=8)
```

**Configuration par d√©faut** :
- **checkpointer=None** : Mode stateless, aucune m√©moire entre questions
- **use_rag_fusion=True** : RAG Fusion activ√© (multi-query + RRF)
- **temperature=1.0** : Temp√©rature maximale pour g√©n√©ration cr√©ative
- **k_documents=8** : Nombre final de documents retourn√©s apr√®s fusion

**Fonctionnement RAG Fusion** :
1. G√©n√®re 4 variations de la question (1 originale + 3 reformulations)
2. R√©cup√®re 4 documents pour chaque variation (total : 16 documents)
3. Applique RRF (Reciprocal Rank Fusion) pour reranker
4. Retourne les top k=8 documents avec meilleurs scores

**LLM** :
- **Mod√®le** : Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
- **Temperature** : 1.0 (configurable)
- **Tools** : Fonction `retrieve` avec RAG Fusion

**Points cl√©s** :
- **Vectorstore** : ChromaDB pass√© en param√®tre (cr√©√© au startup)
- **Checkpointer** : None par d√©faut = AUCUNE m√©moire conversationnelle
- **Tool** : Fonction `retrieve` avec logique RAG Fusion int√©gr√©e
- **Graph** : Compil√© sans checkpointer (stateless)

### 2. Build du Graph LangGraph

```python
def _build_graph(self):
    """Construit le StateGraph LangGraph"""
    workflow = StateGraph(MessagesState)

    # Add nodes
    workflow.add_node("agent", self._call_model)
    workflow.add_node("tools", ToolNode(self.tools))

    # Entry point
    workflow.add_edge(START, "agent")

    # Conditional routing
    def should_continue(state: MessagesState) -> Literal["tools", END]:
        messages = state["messages"]
        last_message = messages[-1]
        # Si tool_calls pr√©sents ‚Üí execute tools
        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
            return "tools"
        # Sinon ‚Üí fin
        return END

    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            END: END,
        }
    )

    # Loop: tools ‚Üí agent
    workflow.add_edge("tools", "agent")

    # Compile (stateless si checkpointer=None)
    if self.checkpointer:
        return workflow.compile(checkpointer=self.checkpointer)
    else:
        return workflow.compile()  # Stateless par d√©faut
```

**Flow** :
1. START ‚Üí agent
2. Agent d√©cide : tool call ou r√©ponse directe
3. Si tool call ‚Üí tools ‚Üí agent (loop)
4. Si pas de tool call ‚Üí END

### 3. Call Model avec Trimming

```python
def _call_model(self, state: MessagesState):
    """Invoke LLM avec message trimming"""
    messages = state["messages"]

    # Trim pour √©viter overflow du contexte
    messages = trim_messages(messages)

    # Ajouter system message si absent
    if not messages or messages[0].type != "system":
        system_msg = SystemMessage(
            "You have access to a tool that retrieves context from documents. "
            "Use the tool to help answer user queries. "
            "IMPORTANT: Provide COMPLETE and COMPREHENSIVE answers."
        )
        messages = [system_msg] + messages

    # Appel LLM avec tools
    response = self.model_with_tools.invoke(messages)
    return {"messages": [response]}
```

**Trimming** :
```python
def trim_messages(messages):
    """Garde les 10 derniers messages pour ne pas d√©passer le contexte"""
    if len(messages) <= 10:
        return messages
    # Garde le premier (system) + 9 derniers
    return [messages[0]] + messages[-9:]
```

### 4. Invoke (Stateless par d√©faut)

Voir le code r√©el dans `backend/rags/rag_agent.py` (lignes 254-288).

**Signature** :
```python
def invoke(self, question: str, thread_id: str = None) -> dict
```

**IMPORTANT** : En mode stateless (checkpointer=None), le param√®tre `thread_id` est **IGNOR√â**.

```python
# Config pour m√©moire (SEULEMENT si checkpointer existe)
config = {}
if thread_id and self.checkpointer:  # <- Note le AND
    config = {"configurable": {"thread_id": thread_id}}
```

**Comportement actuel (checkpointer=None)** :
- Chaque question est **ind√©pendante**
- Aucune m√©moire entre les appels
- thread_id est retourn√© mais n'a aucun effet
- Optimis√© pour √©valuation RAGAS (pas de contamination entre questions)

**Retour** :
```python
{
    "answer": str,           # R√©ponse g√©n√©r√©e
    "messages": list,        # Liste des messages (HumanMessage, AIMessage, ToolMessage)
    "used_retrieval": bool,  # True si le tool retrieve a √©t√© appel√©
    "thread_id": str         # Retourn√© tel quel (tracking frontend uniquement)
}
```

---

## API REST

### Fichier : `backend/api/main.py`

### Startup Event

```python
@app.on_event("startup")
async def startup_event():
    """Initialize RAG agent on startup"""
    global rag_agent, vectorstore

    # Initialize embeddings
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    # Load ChromaDB
    persist_dir = Path(__file__).parent.parent.parent / "data" / "chroma_db"
    vectorstore = Chroma(
        persist_directory=str(persist_dir),
        embedding_function=embeddings
    )

    # Initialize RAG agent (STATELESS mode)
    rag_agent = RAGAgent(vectorstore, checkpointer=None)
    # Par d√©faut : use_rag_fusion=True, temperature=1.0, k_documents=8

    print("‚úì RAG Agent initialized in STATELESS mode with RAG Fusion")
```

### Endpoints Principaux

#### 1. `POST /api/rag_agent`

Endpoint natif pour le RAG Agent.

```python
@app.post("/api/rag_agent", response_model=AgentResponse)
async def query_rag_agent(request: QueryRequest):
    """Query the RAG agent (STATELESS - thread_id ignored)"""
    start_time = time.time()

    try:
        # Generate thread_id si absent (pour tracking frontend uniquement)
        thread_id = request.thread_id or str(uuid.uuid4())

        # Invoke (thread_id IGNOR√â car checkpointer=None)
        result = rag_agent.invoke(request.question, thread_id=thread_id)
        latency = time.time() - start_time

        return AgentResponse(
            answer=result["answer"],
            used_retrieval=result.get("used_retrieval"),
            latency=latency,
            thread_id=result.get("thread_id")
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

**Request** :
```json
{
  "question": "What is RAG?",
  "thread_id": "thread-abc123"  // Optionnel
}
```

**Response** :
```json
{
  "answer": "RAG stands for...",
  "used_retrieval": true,
  "latency": 2.45,
  "thread_id": "thread-abc123"
}
```

#### 2. `POST /api/query`

Endpoint adapter pour compatibilit√© frontend.

```python
@app.post("/api/query")
async def query_adapter(request: dict):
    """Adapter pour frontend original (STATELESS)"""
    question = request.get("question", "")
    session_id = request.get("session_id", str(uuid.uuid4()))

    # Mapping session_id ‚Üí thread_id (INUTILE car checkpointer=None)
    # Gard√© uniquement pour compatibilit√© frontend
    if session_id not in session_threads:
        session_threads[session_id] = f"thread-{uuid.uuid4()}"
    thread_id = session_threads[session_id]

    # Invoke agent (thread_id IGNOR√â)
    result = rag_agent.invoke(question, thread_id=thread_id)

    return {
        "answer": result["answer"],
        "method": "optimized_rag_agent",
        "latency": ...,
        "confidence": 0.90,  # Hardcod√©
        "faithfulness": 0.92,  # Hardcod√©
        "thread_id": thread_id  # Retourn√© mais sans effet
    }
```

**Notes** :
- `confidence` et `faithfulness` sont **hardcod√©s**
- Le mapping session_id ‚Üí thread_id est **inutile** (checkpointer=None) mais gard√© pour compatibilit√©

#### 3. `POST /api/upload`

Upload et traitement de documents.

```python
@app.post("/api/upload")
async def upload_documents(files: List[UploadFile] = File(...)):
    """Upload PDF, TXT, MD, DOCX, IPYNB"""

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,
        chunk_overlap=400
    )

    for file in files:
        # Save temp
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            content = await file.read()
            tmp.write(content)
            tmp_path = tmp.name

        # Load selon extension
        if file.filename.endswith('.pdf'):
            loader = PyPDFLoader(tmp_path)
        elif file.filename.endswith(('.txt', '.md')):
            loader = TextLoader(tmp_path)
        elif file.filename.endswith('.docx'):
            loader = UnstructuredWordDocumentLoader(tmp_path)
        elif file.filename.endswith('.ipynb'):
            loader = NotebookLoader(tmp_path)

        documents = loader.load()

        # Split
        chunks = text_splitter.split_documents(documents)

        # Add metadata
        for chunk in chunks:
            chunk.metadata["source"] = file.filename
            chunk.metadata["upload_date"] = time.strftime("%Y-%m-%d %H:%M:%S")

        # Add to vectorstore
        vectorstore.add_documents(chunks)

    return {"total_chunks": total_chunks}
```

---

## Mode Stateless (Configuration Actuelle)

### Pas de M√©moire Conversationnelle

Le syst√®me est configur√© en **mode stateless** par d√©faut (`checkpointer=None`).

**Implications** :
- ‚ùå **Aucune m√©moire** entre les questions
- ‚ùå Le param√®tre `thread_id` est **accept√© mais IGNOR√â**
- ‚úÖ Chaque question est **totalement ind√©pendante**
- ‚úÖ Optimis√© pour **√©valuation RAGAS** (pas de contamination entre questions)
- ‚úÖ Pas de stockage d'√©tat en m√©moire

**Exemple de comportement actuel** :

```python
# Question 1
agent.invoke("My name is Alice", thread_id="thread-1")
# ‚Üí R√©ponse g√©n√©r√©e

# Question 2 (M√äME thread_id)
agent.invoke("What is my name?", thread_id="thread-1")
# ‚Üí "I don't have information about your name" ‚ùå Pas de m√©moire !

# Le thread_id ne fait RIEN car checkpointer=None
```

### Activer la M√©moire Conversationnelle (Non impl√©ment√© actuellement)

Pour activer la m√©moire, il faudrait :

1. **Changer le code dans `backend/api/main.py`** :
```python
from langgraph.checkpoint.memory import MemorySaver

# Ligne 77 (actuellement : checkpointer=None)
checkpointer = MemorySaver()  # Au lieu de None
rag_agent = RAGAgent(vectorstore, checkpointer=checkpointer)
```

2. **Fonctionnement avec m√©moire** :
- Chaque `thread_id` aurait son propre √©tat isol√©
- L'historique des messages serait sauvegard√©
- Pas de limite de dur√©e (reste en RAM)

3. **Pour persistance en production** :
- Utiliser `SqliteSaver` ou `PostgresSaver` au lieu de `MemorySaver`
- Voir [LangGraph Checkpointers](https://langchain-ai.github.io/langgraph/reference/checkpoints/)

**Note** : La m√©moire n'est PAS activ√©e pour optimiser l'√©valuation RAGAS.

---

## Streaming

### Server-Sent Events (SSE)

Endpoint streaming pour r√©ponses en temps r√©el.

```python
@app.post("/api/query/stream")
async def query_stream(request: dict):
    """Streaming SSE"""
    question = request.get("question", "")
    thread_id = ...

    return StreamingResponse(
        generate_sse_stream(question, thread_id),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",
        }
    )
```

### G√©n√©rateur SSE

```python
async def generate_sse_stream(question: str, thread_id: str):
    """Generate SSE events"""

    # Get full response
    result = rag_agent.invoke(question, thread_id)
    full_response = result["answer"]

    # Simulate streaming (mot par mot)
    words = full_response.split()
    for word in words:
        event_data = {
            "type": "token",
            "content": word + " ",
            "thread_id": thread_id
        }
        yield f"data: {json.dumps(event_data)}\n\n"
        await asyncio.sleep(0.03)  # 30ms delay

    # Completion event
    event_data = {
        "type": "complete",
        "content": full_response,
        "thread_id": thread_id
    }
    yield f"data: {json.dumps(event_data)}\n\n"
```

**Note** : LangGraph ne supporte pas encore le streaming token-by-token natif, d'o√π la simulation.

---

## Upload de Documents

### Formats Support√©s

- **PDF** : `PyPDFLoader`
- **TXT/MD** : `TextLoader`
- **DOCX** : `UnstructuredWordDocumentLoader`
- **IPYNB** : `NotebookLoader` (avec outputs)

### Text Splitting

```python
RecursiveCharacterTextSplitter(
    chunk_size=2000,      # Chunks plus gros pour meilleur contexte
    chunk_overlap=400,    # 20% overlap
    separators=["\n\n", "\n", " ", ""]
)
```

### Ajout Vectorstore

```python
# Add chunks avec m√©tadonn√©es
vectorstore.add_documents(chunks)

# ChromaDB persiste automatiquement
```

---

## Tests

### Test Mode Stateless

**Fichier** : `test_memory.py`

```bash
python test_memory.py
```

**Tests actuels (mode stateless)** :
- ‚ùå Threads diff√©rents ‚Üí pas de m√©moire (normal)
- ‚ùå M√™me thread ‚Üí **PAS de m√©moire** (car checkpointer=None)
- ‚úÖ Trimming ‚Üí garde 10 messages max
- ‚úÖ RAG Fusion ‚Üí g√©n√®re 4 queries, r√©cup√®re 16 docs, retourne top 8

### Test API Manuel

```bash
# Lancer serveur
cd backend/api && python main.py

# Test basique (stateless)
curl -X POST http://localhost:8000/api/rag_agent \
  -H "Content-Type: application/json" \
  -d '{"question": "What is RAG?"}'

# Test avec thread_id (IGNOR√â car checkpointer=None)
curl -X POST http://localhost:8000/api/rag_agent \
  -H "Content-Type: application/json" \
  -d '{"question": "My name is Bob", "thread_id": "test-1"}'

curl -X POST http://localhost:8000/api/rag_agent \
  -H "Content-Type: application/json" \
  -d '{"question": "What is my name?", "thread_id": "test-1"}'
# ‚Üí "I don't have information about your name" ‚ùå PAS de m√©moire !
# Le thread_id est accept√© mais IGNOR√â (checkpointer=None)
```

**Note** : Pour tester la m√©moire, il faudrait d'abord modifier `backend/api/main.py` ligne 77 pour activer un checkpointer.

---

## D√©ploiement

### Variables d'Environnement

```bash
# .env
ANTHROPIC_API_KEY=sk-ant-...   # Requis
TAVILY_API_KEY=tvly-...        # Optionnel (non utilis√©)
```

### Docker Compose

```yaml
version: '3.8'

services:
  backend:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
    volumes:
      - ./data:/app/data

  frontend:
    build: ./frontend
    ports:
      - "7860:80"
    depends_on:
      - backend
```

### Lancement

```bash
docker-compose up --build
```

**Acc√®s** :
- Backend : http://localhost:8000
- Frontend : http://localhost:7860
- Docs API : http://localhost:8000/docs

---

## M√©triques

### Actuelles (Hardcod√©es)

Dans `/api/query` adapter :
```python
"confidence": 0.90,
"faithfulness": 0.92,
```

### Futures (RAGAS)

Pour de vraies m√©triques :
- **Faithfulness** : LLM-as-judge hallucination
- **Answer Relevance** : Pertinence question-r√©ponse
- **Context Precision** : Documents utilis√©s vs r√©cup√©r√©s

Voir plan d'√©valuation s√©par√©.

---

## S√©curit√©

### API Key

```python
# Jamais en dur !
api_key = os.getenv("ANTHROPIC_API_KEY")
if not api_key:
    raise RuntimeError("ANTHROPIC_API_KEY not found")
```

### Input Validation

```python
class QueryRequest(BaseModel):
    question: str
    thread_id: Optional[str] = None

    @validator('question')
    def validate_question(cls, v):
        if not v or len(v) > 5000:
            raise ValueError("Invalid question length")
        return v.strip()
```

### CORS

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # √Ä restreindre en prod
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

---

## Ressources

- [LangChain RAG Agent Tutorial](https://python.langchain.com/docs/tutorials/rag_agent/)
- [LangGraph Docs](https://langchain-ai.github.io/langgraph/)
- [Anthropic Claude API](https://docs.anthropic.com/)
- [ChromaDB Docs](https://docs.trychroma.com/)

---

## Roadmap

### Impl√©ment√© ‚úÖ
- **RAG Agent** avec tool calling (LangGraph)
- **RAG Fusion** : Multi-query retrieval + RRF reranking (4 queries ‚Üí 16 docs ‚Üí top 8)
- **Mode stateless** : Pas de m√©moire conversationnelle (optimis√© pour √©valuation)
- **Claude Sonnet 4.5** : Mod√®le de derni√®re g√©n√©ration
- **Temperature 1.0** : G√©n√©ration cr√©ative maximale
- **Streaming SSE** : R√©ponses en temps r√©el
- **Upload multi-formats** : PDF/TXT/MD/DOCX/IPYNB
- **Message trimming** : Garde 10 derniers messages
- **API REST compl√®te** : FastAPI avec CORS
- **√âvaluation RAGAS** : Score 87.4% - Grade A

### √Ä Venir üîú
- **Mode conversationnel** avec m√©moire (MemorySaver/PostgresSaver)
- **Hybrid search** : Dense + sparse (BM25)
- **Citation tracking** : Sources pr√©cises
- **Token usage tracking** : Co√ªts r√©els
- **Rate limiting** : Protection API
- **Tests automatis√©s** : Coverage complet
